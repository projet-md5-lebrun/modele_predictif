{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Xgboost\n",
    "I use my brain to explore the Xgboost algorithm,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce epoch and depth\n",
    "\n",
    "- `max_depth` is set to 3 instead of 4 to simplify the tree structure.\n",
    "- `epochs` is reduced to 5, meaning the boosting process stops after 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=0)\n",
    "\n",
    "# Convert data into DMatrix format for XGBoost\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters for XGBoost model\n",
    "param = {\n",
    "    'max_depth': 3,    # Try reducing max_depth\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3\n",
    "} \n",
    "\n",
    "# Reduce the number of boosting rounds (epochs)\n",
    "epochs = 5  # Try reducing the number of epochs\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = xgb.train(param, train, epochs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "# Define new parameters for XGBoost model\n",
    "param = {\n",
    "    'max_depth': 3,    \n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3\n",
    "} \n",
    "\n",
    "# Reduce the number of boosting rounds (epochs)\n",
    "epochs = 1  \n",
    "model = xgb.train(param, train, epochs)\n",
    "\n",
    "predictions = model.predict(test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The high accuracy of approximately 96.7% on the Iris dataset suggests good model performance, but the slightly lower test accuracy compared to training accuracy hints at potential overfitting, where the model may not generalize optimally to new data.\n",
    "\n",
    "To address overfitting:\n",
    "\n",
    "- Use techniques like cross-validation to evaluate performance on different data subsets and ensure consistent model performance.\n",
    "- Experiment with simpler model configurations by reducing max_depth and adjusting other parameters to strike a balance between model complexity and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for XGBoost model\n",
    "param = {\n",
    "    'max_depth': 10,    # Set a very low max_depth\n",
    "    'eta': 0.1,        # Lower the learning rate\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3\n",
    "} \n",
    "\n",
    "epochs = 1  # Use only 1 boosting round\n",
    "model = xgb.train(param, train, epochs)\n",
    "\n",
    "predictions = model.predict(test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.7, random_state=42)\n",
    "\n",
    "# Convert data into DMatrix format for XGBoost\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters for XGBoost model\n",
    "param = {\n",
    "    'max_depth': 3,    # Try reducing max_depth\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3\n",
    "} \n",
    "\n",
    "# Reduce the number of boosting rounds (epochs)\n",
    "epochs = 1  # Try reducing the number of epochs\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = xgb.train(param, train, epochs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The achieved accuracy of approximately 91.4% on a large test size (70%) suggests potential underfitting due to reduced model complexity (low max_depth and 1 epoch). Adjusting parameters and exploring a more balanced dataset split can help optimize the model's generalization and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to detect overfitting ?\n",
    "If train_accuracy > val_accuracy then the model is overfitting. Because the model is learning the training data too well and may not generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "Validation Accuracy: 0.9000\n",
      "Test Accuracy: 1.0000\n",
      "Warning: Potential overfitting detected.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)  # 60% train, 20% validation, 20% test\n",
    "\n",
    "# Convert data into DMatrix format for XGBoost\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "val = xgb.DMatrix(X_val, label=y_val)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters for XGBoost model\n",
    "param = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3\n",
    "} \n",
    "\n",
    "# Train the XGBoost model on the training set\n",
    "epochs = 100\n",
    "model = xgb.train(param, train, epochs, evals=[(train, 'train'), (val, 'validation')], early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_predictions = model.predict(train)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = model.predict(val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "\n",
    "# Evaluate the model on the test set (unseen data)\n",
    "test_predictions = model.predict(test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Check if there's a large difference between training and validation accuracies\n",
    "if train_accuracy > val_accuracy:\n",
    "    print(\"Warning: Potential overfitting detected.\")\n",
    "else:\n",
    "    print(\"Model is generalizing well.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The training accuracy of 100% combined with a lower validation accuracy of 90% and perfect test accuracy of 100% indicates potential overfitting, where the model is likely memorizing the training data but not generalizing well to unseen validation and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8935\n",
      "Validation Accuracy: 0.7532\n",
      "Test Accuracy: 0.8312\n",
      "Warning: Potential overfitting detected.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the diabetes dataset from CSV file\n",
    "diabetes = pd.read_csv('../data/diabetes.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = diabetes.drop('Outcome', axis=1)\n",
    "y = diabetes['Outcome']\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)  # 60% train, 20% validation, 20% test\n",
    "\n",
    "# Convert data into DMatrix format for XGBoost\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "val = xgb.DMatrix(X_val, label=y_val)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters for XGBoost model\n",
    "param = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'binary:logistic',  # Use binary classification objective\n",
    "    'eval_metric': 'error'  # Specify evaluation metric as classification error\n",
    "}\n",
    "\n",
    "# Reduce the number of boosting rounds (epochs)\n",
    "epochs = 50  # Use more epochs for better performance\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = xgb.train(param, train, epochs, evals=[(train, 'train'), (val, 'validation')], early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "# Predict classes (0 or 1) on the training, validation, and test sets\n",
    "train_predictions = model.predict(train)\n",
    "val_predictions = model.predict(val)\n",
    "test_predictions = model.predict(test)\n",
    "\n",
    "# Convert predicted probabilities to class labels (0 or 1)\n",
    "train_pred_labels = [1 if pred > 0.5 else 0 for pred in train_predictions]\n",
    "val_pred_labels = [1 if pred > 0.5 else 0 for pred in val_predictions]\n",
    "test_pred_labels = [1 if pred > 0.5 else 0 for pred in test_predictions]\n",
    "\n",
    "# Calculate accuracy scores for training, validation, and test sets\n",
    "train_accuracy = accuracy_score(y_train, train_pred_labels)\n",
    "val_accuracy = accuracy_score(y_val, val_pred_labels)\n",
    "test_accuracy = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "# Print accuracy scores\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Check for potential overfitting (compare training accuracy with validation accuracy)\n",
    "if train_accuracy > val_accuracy:\n",
    "    print(\"Warning: Potential overfitting detected.\")\n",
    "else:\n",
    "    print(\"Model is generalizing well.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'eta': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0, 'objective': 'binary:logistic', 'eval_metric': 'error'}\n",
      "Best Validation Accuracy: 0.7922077922077922\n",
      "Final Test Accuracy: 0.8246753246753247\n",
      "Model is generalizing well.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the diabetes dataset from CSV file\n",
    "diabetes = pd.read_csv('../data/diabetes.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = diabetes.drop('Outcome', axis=1)\n",
    "y = diabetes['Outcome']\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)  # 60% train, 20% validation, 20% test\n",
    "\n",
    "# Convert data into DMatrix format for XGBoost\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "val = xgb.DMatrix(X_val, label=y_val)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define a grid of parameters to search over\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'eta': [0.1, 0.3, 0.5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "best_params = None\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "# Try different combinations of parameters using nested loops\n",
    "for max_depth in param_grid['max_depth']:\n",
    "    for eta in param_grid['eta']:\n",
    "        for subsample in param_grid['subsample']:\n",
    "            for colsample_bytree in param_grid['colsample_bytree']:\n",
    "                params = {\n",
    "                    'max_depth': max_depth,\n",
    "                    'eta': eta,\n",
    "                    'subsample': subsample,\n",
    "                    'colsample_bytree': colsample_bytree,\n",
    "                    'objective': 'binary:logistic',  # Use binary classification objective\n",
    "                    'eval_metric': 'error'  # Specify evaluation metric as classification error\n",
    "                }\n",
    "                \n",
    "                # Train the XGBoost model with current parameters\n",
    "                model = xgb.train(params, train, num_boost_round=100, evals=[(val, 'validation')], early_stopping_rounds=10, verbose_eval=False)\n",
    "                \n",
    "                # Evaluate the model on the validation set\n",
    "                val_predictions = model.predict(val)\n",
    "                val_pred_labels = [1 if pred > 0.5 else 0 for pred in val_predictions]\n",
    "                val_accuracy = accuracy_score(y_val, val_pred_labels)\n",
    "                \n",
    "                # Check if current parameters yield better validation accuracy\n",
    "                if val_accuracy > best_val_accuracy:\n",
    "                    best_val_accuracy = val_accuracy\n",
    "                    best_params = params\n",
    "\n",
    "# Print the best parameters and their corresponding validation accuracy\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Validation Accuracy:\", best_val_accuracy)\n",
    "\n",
    "# Train the final model using the best parameters on the combined train + validation set\n",
    "final_train = xgb.DMatrix(pd.concat([X_train, X_val]), label=pd.concat([y_train, y_val]))\n",
    "final_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "final_model = xgb.train(best_params, final_train, num_boost_round=100)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_predictions = final_model.predict(final_test)\n",
    "test_pred_labels = [1 if pred > 0.5 else 0 for pred in test_predictions]\n",
    "test_accuracy = accuracy_score(y_test, test_pred_labels)\n",
    "\n",
    "print(\"Final Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Check for potential overfitting (compare training + validation accuracy with test accuracy)\n",
    "if (best_val_accuracy > test_accuracy):\n",
    "    print(\"Warning: Potential overfitting detected.\")\n",
    "else:\n",
    "    print(\"Model is generalizing well.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
